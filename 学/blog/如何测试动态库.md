前一阵子给xgboost4j加了个接口，直接从稀疏向量预测，避免构造DMatrix，这样可以大大减轻GC的压力。
```java
float[] predictInst(int[] indices, float[] data, boolean outputMargin, int treeLimit)
```
在写jni实现时，不小心埋下一个坑。
```cpp
int* indices = static_cast<int*>(jenv->GetPrimitiveArrayCritical(jindices, 0));
float* data = static_cast<float*>(jenv->GetPrimitiveArrayCritical(jindices, 0));
```
正确的应该是这样
```cpp
int* indices = static_cast<int*>(jenv->GetPrimitiveArrayCritical(jindices, 0));
float* data = static_cast<float*>(jenv->GetPrimitiveArrayCritical(jdata, 0));
```
这个坑我后来发现了，但是由于当时同时处理的分支比较多，可能忘记提交上了。于是我一直以为自己修改过了，并且添加的测试也是通过了的。
```java
 @Test
  public void testPredictInst() throws XGBoostError, IOException {

    DMatrix trainMat = new DMatrix("../../demo/data/agaricus.txt.train");
    DMatrix testMat = new DMatrix("../../demo/data/agaricus.txt.test");

    Booster booster = trainBooster(trainMat, testMat);
    //predict raw output
    float[][] predicts1 = booster.predict(testMat, false, 0);

    // Scanner in = new Scanner(Paths.get("../../demo/data/agaricus.txt.onerow"), "UTF-8");
    Scanner in = new Scanner(Paths.get("../../demo/data/agaricus.txt.test"), "UTF-8");

    List<float[]> preidcts2 = new ArrayList<>(1611);
    while (in.hasNextLine()) {
      String[] fileds = in.nextLine().split(" ");
      int[] indices = new int[fileds.length - 1];
      float[] data = new float[fileds.length - 1];
      for (int i = 1; i < fileds.length; i++) {
        String[] pair = fileds[i].split(":");
        indices[i - 1] = Integer.parseInt(pair[0]);
        data[i - 1] = Float.parseFloat(pair[1]);
      }
      float[] result = booster.predictInst(indices, data, true, 0);
      preidcts2.add(result);
    }

    for (int i = 0; i < predicts1.length; i++) {
      for (int j = 0; j < predicts1[i].length; j++) {
        Assert.assertEquals(predicts1[i][j], preidcts2.get(i)[j], 0.00001);
      }

    }
  }
```
测试没毛病，但是给业务开发测试就是有问题，这就很尴尬了。刚开始直接怀疑是cpp内部调用出问题了，于是加printf语句，打印传入的值，还是通过mvn test跑测试。按道理这里应该发现的问题，可是也没发现哪里有问题，可能分支搞错乱了。最重要的是我受不了Java这个修改测试迭代速度。

昨天花了一点时间看了下Python接口封装，仿照添加了我的接口。
```python
 def predictInst(self, indices, values, output_margin=False, ntree_limit=0):
        length = ctypes.c_int()
        preds = ctypes.POINTER(ctypes.c_float)()
        _check_call(_LIB.XGBoosterPredict(self.handle, 
                                          ctypes.c_int(len(indices)),
                                          ctypes.byref((ctypes.c_int * len(indices))(*indices)),
                                          ctypes.byref((ctypes.c_float * len(values))(*values)),
                                          ctypes.c_bool(output_margin),
                                          ctypes.c_int(ntree_limit),
                                          ctypes.byref(length),
                                          ctypes.byref(preds)))
        preds = ctypes2numpy(preds, length.value, np.float32)
        return preds
```
最后在IPython环境下测试，舒服多了，更多的精力专注在问题上，很快确认c api没问题。于是回过头来看jni接口封装，发现了上述的坑。

再次感慨：人生苦短，我用Python！
